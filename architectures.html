<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Architectures</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="architectures.html">Architectures</a>
</li>
<li>
  <a href="optimization_routines.html">Optimization Routines</a>
</li>
<li>
  <a href="learning_rate_scheduling.html">Learning Rate Scheduling</a>
</li>
<li>
  <a href="regularization.html">Regularization</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Architectures</h1>

</div>


<div id="recurrent-neural-networks-rnns" class="section level2">
<h2>Recurrent Neural Networks (RNNs)</h2>
<div id="recurrent-neural-networks" class="section level3">
<h3>Recurrent Neural Networks</h3>
<p>While the paradigm in which there is a fixed set of variables <span class="math inline">\(x_1,...,x_d\)</span> used to make a prediction for an outcome <span class="math inline">\(y\)</span> of fixed dimension (discrete, continuous, vector, etc.) is the primary focus of classical machine learning, some tasks involve making predictions with or over sequences of (often) dependent data.</p>
<p>For example, given an audio file containing bird calls, one might want to predict the bird species.</p>
<p><img src="images/bird_call_prediction.png" /></p>
<p>Or perhaps, one might want to generate a sequence of musical notes given a starting point.</p>
<p><img src="images/music_prediction.png" /></p>
<p>In these examples, the inputs and outputs of a prediction model can be of variable length and are dependent. Recurrent Neural Networks (RNNs) are a special type of architecture designed to address these tasks.</p>
</div>
<div id="architecture" class="section level3">
<h3>Architecture</h3>
<p><img src="images/rnn_basic.png" /></p>
<p>Let us first consider a traditional RNN architecture in its “unrolled” depiction. Given a starting hidden state <span class="math inline">\(h^0\)</span> and an input <span class="math inline">\(x^1\)</span>, the network updates to a new hidden state <span class="math inline">\(h_1\)</span> and makes a prediction <span class="math inline">\(\hat{y}^1\)</span>. Then, the hidden state <span class="math inline">\(h_1\)</span> is passed along as input to generate the next hidden state <span class="math inline">\(h_2\)</span>. Written out mathematically, the model is defined recursively:</p>
<p><span class="math display">\[
\hat{y}^{(t+1)} = g_y(W_{yh} h^{(t)} + b_{y}) 
\]</span> where <span class="math display">\[
h^{(t)} = g_h(W_{hh} h^{(t-1)} + W_{hx} x^{(t)} + b_h).
\]</span></p>
<p>From a statistical perspective, the structure of this model assumes that <span class="math inline">\(h^{(t)}\)</span> contains all the information needed from the past to predict <span class="math inline">\(y^{t}\)</span> and future elements in the sequence (a Markov-like property).</p>
<p>Recurrent neural networks are flexible models, with a traditional neural network actually being just a special case, A few of the more commonly used architectures are displayed below.</p>
<p><img src="images/rnn_examples.png" /></p>
</div>
<div id="optimizing-rnns" class="section level3">
<h3>Optimizing RNNs</h3>
<p>While the recurrence relation in RNNs allows for the passing of information from previous states to future states and predictions through the hidden states, a practical downside is that this makes optimization very challenging numerically. For simplicty sake, let us assume write our hidden state as <span class="math inline">\(h^t = g_h(x^t, h^{t-1}, w_h)\)</span> and <span class="math inline">\(\hat{y}^t = g_y(h^t, w_y)\)</span>, where our parameters to learn are <span class="math inline">\(w_h\)</span> and <span class="math inline">\(w_y\)</span>. It can be shown that the gradient of the loss of a RNN for one sequence of data points has the following form:</p>
<p><span class="math display">\[
\dfrac{\partial L}{\partial w_h} = \dfrac{1}{T} \sum_{t=1}^T \dfrac{\partial L}{\partial \hat{y}^t} \dfrac{\partial g_y(h^t, w_y)}{\partial h^t} \big[ \dfrac{\partial g_h(x^t, h^{t-1}, w_h)}{\partial w_h} + \sum_{i &lt; t} \big( \prod_{j = i + 1}^t  \dfrac{\partial g_h(x^j, h^{j-1}, w_h)}{\partial h^{j-1}}\big) \dfrac{\partial g_h(x^i, h^{i-1}, w_h)}{\partial w_h} \big]
\]</span></p>
<p>The term <span class="math inline">\(\prod_{j = i + 1}^t \dfrac{\partial g_h(x^j, h^{j-1}, w_h)}{\partial h^{j-1}}\)</span> involves the product of up to <span class="math inline">\(T\)</span> matrices. For long sequences, this is numerically unstable, which can lead to a phenomena called exploding or vanishing gradients. Practical remedies to this include gradient clipping (thresholding the norm of the gradient at a certain large value) or truncating the number of products.</p>
</div>
<div id="lstms" class="section level3">
<h3>LSTMs</h3>
<p>Example from this blog (<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="uri">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>).</p>
<p>LSTMs are a special kind of recurrent neural network designed to learn long-term dependencies in the data. We know RNN’s are used to handle sequential data, and can handle some sense of dependence on previous (sequential or temporal data). However, problems arise when the dependence occurs too far in the past (causing vanishing gradients).</p>
<p>Take for example the sentence “the clouds are in the sky.” The word “sky” can be inferred from “clouds” earlier in the sentence, but what if the sentence was “I grew up in France…” Followed by a few sentences, and then “I speak French.” It would be difficult for a traditional RNN to draw on this previous information that far back. LSTMs were designed to solve this “long-term” dependency problem, in which “remembering” information for long periods is the default.</p>
<p><img src="images/rnn_comparison.png" /></p>
<p>Above is an example of a “short-term” dependency that RNN’s were designed to handle. However, here is a second example of a “long-term” dependency that traditional RNN’s fail to pick up on.</p>
<p><img src="images/long_term.png" /></p>
<p>LSTM’s add an internal state (cell) to an RNN node, in addition to the input, as well as receiving the output as input. The cell consists of a forget gate, input gate, and an output gate (total of 3). Forget says the information in the internal state can be forgotten, input gate says what new input should be added, and output defines which parts of the state should be output (values can be assigned to each gate between 0-1, where 0 is completely closed). Now we look at a walkthrough of this with images below taken from (<a href="https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e" class="uri">https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e</a>).</p>
</div>
<div id="walkthrough" class="section level3">
<h3>Walkthrough:</h3>
<p><img src="images/lstm_architecture.png" /></p>
<p>Above is an example of a basic RNN where the hidden nodes are recurrent units. Zooming in on one of the recurrent units:</p>
<p><img src="images/recurrent_unit.png" /></p>
<p>It can be seen that <span class="math inline">\(h_{t-1}\)</span>, the hidden state from a previous time step, is fed back into the node along with the input at the current timestep <span class="math inline">\(x_t\)</span> as is in a traditional RNN. In the LSTM model, this is still true, but we add a new hidden state into the node, along with the three forget, input, and output gates.</p>
<p><img src="images/ltsm_unit.png" /></p>
<p>The sigmoid activation functions set a weight of [0,1] to the input and output candidates, which are ultimately combined and passed to determine the new hidden state and output. It can be seen the hidden state from previous timestep <span class="math inline">\(h_{t-1}\)</span> is combined with the input at the current time step <span class="math inline">\(x_t\)</span> as in an RNN, but this data is then fed into the forget and input gates to modify the cell state from <span class="math inline">\(c_{t-1}\)</span> to a new present cell state <span class="math inline">\(c_{t}\)</span>, meanwhile the output gate determines what is fed back into the network.</p>
</div>
<div id="grus" class="section level3">
<h3>GRUs</h3>
<p>A gated recurrent unit (GRU) functions similarly to an LSTM, only instead of the three gates in LSTM (forget, input, output), these are replaced by just two (reset, update). These gates similarly have sigmoid activation functions setting the values between [0,1]. The reset gate can be thought of as how much of the previous state should be retained, meanwhile the update gate controls how much of current state is copied to the new state. Images taken from (<a href="https://d2l.ai/chapter_recurrent-modern/gru.html" class="uri">https://d2l.ai/chapter_recurrent-modern/gru.html</a>).</p>
<p><img src="images/reset_gate.png" /></p>
<p><img src="images/reset_gate2.png" /></p>
<p>Intuitively, reset gates help capture short-term dependencies, while update gates help capture long-term dependencies.</p>
<div id="references" class="section level4">
<h4>References</h4>
<p>Much of these notes were adapted from the following excellent resources:</p>
<p><a href="https://d2l.ai/chapter_recurrent-modern/gru.html" class="uri">https://d2l.ai/chapter_recurrent-modern/gru.html</a></p>
<p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" class="uri">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks</a></p>
</div>
</div>
</div>
<div id="autoencoders" class="section level2">
<h2>Autoencoders</h2>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>An autoencoder is a neural network (with input, hidden/bottleneck, and output layer) which attempts to copy the input, as closely as possible, to the output. This falls into a category of <strong>unsupervised representation learning</strong>, as the bottleneck layer forces a lower-dimensional representation of the original input. This framework can handle data which has structure amongst its features (e.g. time-correlated data, images, etc.). If the output layer is a good reconstruction of the input layer, the hidden layer may be an adequate compressed-knowledge representation.</p>
<p>Let <span class="math inline">\(g\)</span> take the input, <span class="math inline">\(X\)</span>, to the hidden layer (so <span class="math inline">\(g(X)\)</span> represents the hidden layer/latent space), and let <span class="math inline">\(r\)</span> take the hidden layer to the output (so <span class="math inline">\(r\{ g(X) \}\)</span> represents the output layer).</p>
<div class="figure" style="text-align: center">
<img src="images/undercomplete.png" alt="An undercomplete autoencoder." width="50%" />
<p class="caption">
An undercomplete autoencoder.
</p>
</div>
</div>
<div id="motivating-example" class="section level3">
<h3>Motivating Example</h3>
<p>Data from continuous glucose monitoring (CGM) in diabetes consists of a blood glucose reading every few minutes for periods of time which can span months. This data is thus hyperstructured over time, with complex relationships and time correlations, from which features must be extracted in order to be useful for prediction or decision-making analysis pipelines.</p>
<p>Many human-engineered features are known to have clinical and analytical value: time in hypoglycemia (proportion of readings with blood sugar &lt; 70 mg/dL), glucose coefficient of variation (a measure of volatility of blood sugar readings), etc. However, if only these known, summary features are extracted from CGM data, various potentially useful parts of CGM trajectories may be wasted. Reconstructing CGM trajectories through a neural network architecture consisting of a hidden middle layer may pave a path for going beyond human-engineered features and computationally discovering relevant features from such complex data.</p>
<div class="figure" style="text-align: center">
<img src="images/cgm.png" alt="An example of a portion of a CGM data trajectory, depicted in Prendin et al. (2021)." width="50%" />
<p class="caption">
An example of a portion of a CGM data trajectory, depicted in Prendin et al. (2021).
</p>
</div>
</div>
<div id="types-of-autoencoders" class="section level3">
<h3>Types of Autoencoders</h3>
<p>There are a variety of different autoencoders, which serve different purposes. We discuss a few types here.</p>
<div id="undercomplete-autoencoder" class="section level4">
<h4>Undercomplete Autoencoder</h4>
<p>This is the most intuitive version of an autoencoder. Directly copying the input to the output has no value, so we insert a hidden layer with smaller dimension than the input layer - this is the definition of an undercomplete autoencoder. This forces the autoencoder to prioritize and capture the most representative features of the input data in reconstruction of the output.</p>
<p>From this architecture, the learning process strives to minimize the loss function, <span class="math inline">\(L[X, r\{ g(X) \}]\)</span>. <span class="math inline">\(L\)</span> penalizes the dissimilarity between <span class="math inline">\(X\)</span> and <span class="math inline">\(r\{ g(X) \}\)</span> (such as MSE).</p>
</div>
<div id="regularized" class="section level4">
<h4>Regularized Autoencoder</h4>
<p>In the undercomplete setting, the idea is that the autoencoder learns useful representations because: (1) The encoder, <span class="math inline">\(g\)</span>, and decoder, <span class="math inline">\(r\)</span>, are kept shallow, and (2) The size of the hidden layer is kept small.</p>
<p>Regularized autoencoders provide useful representations through another method: A loss function which encourages that the model has other properties in addition to simply copying input to output (e.g. sparsity of representation, robustness to noise, etc.). Specifically, a sparse autoencoder minimizes <span class="math inline">\(L[X, r\{ g(X) \}] + \lambda\{g(X)\}\)</span>, where <span class="math inline">\(\lambda\{g(X)\}\)</span> is a sparsity penalty on the hidden layer, and a denoising autoencoder minimizes <span class="math inline">\(L[X, r\{ g(\tilde{X}) \}]\)</span>, where <span class="math inline">\(\tilde{X}\)</span> is a corrupted/noisy version of <span class="math inline">\(X\)</span>.</p>
<p>Therefore, a regularized autoencoder does not necessarily need to be undercomplete. Whereas as undercomplete autoencoder model learns useful representations by through constraints in the neural network architecture, a regularized autoencoder learns useful representations through constrains on the model imposed by the loss function.</p>
</div>
<div id="variational-autoencoder" class="section level4">
<h4>Variational Autoencoder</h4>
<p>An autoencoder, in its standard implementation, is not generative. That is, the latent space is not organized in such a way that a random extrapolation point in the space will map to an output that makes sense. In order to use the latent space of the autoencoder for generative purposes, it needs to be “regular” enough.</p>
<p>A variational autoencoder (VAE) builds the latent space in a probabilistic manner (each latent state attribute is a probability distribution), so that even parts of the space outside of the range of observed data can map to new outputs in a reasonable manner.</p>
<p>The following figure depicts a VAE which turns discrete molecules into a continuous latent space representation. At the same time, the optimizer moves through the latent space to find new molecules which are estimated to maximize the values of certain properties of interest.</p>
<div class="figure" style="text-align: center">
<img src="images/vae.png" alt="VAE architecture used in Gomez-Bombarelli et al. (2018)." width="50%" />
<p class="caption">
VAE architecture used in Gomez-Bombarelli et al. (2018).
</p>
</div>
</div>
</div>
<div id="challenges" class="section level3">
<h3>Challenges</h3>
<div id="imperfect-decoding-threshold" class="section level4">
<h4>Imperfect Decoding Threshold</h4>
<p>It may appear arbitrary to decide what testing MSE constitutes a “good” autoencoder from which the hidden layer can be used as new features, especially when the input is extremely high-dimensional. Determining acceptable levels of decoding error may also depend on the application and risk tolerance.</p>
</div>
<div id="finding-the-right-architecture" class="section level4">
<h4>Finding the Right Architecture</h4>
<p>A bottleneck layer that is too narrow may result in algorithms that miss important dimensions of the input data. On the other hand, an overly generous bottleneck layer may result in the input being more-or-less copied and pasted to the output through the middle layer (overfitting), rather than learning a low-dimensional representation with key features. Introducing loss regularization techniques, such as those in Section 3.2, may help with this issue.</p>
</div>
<div id="insufficient-data" class="section level4">
<h4>Insufficient Data</h4>
<p>Since autoencoders are unsupervised and learn from the input data directly (rather than human-created labels), they often require a considerable amount of clean data. It may be difficult to determine the required amount of data.</p>
</div>
</div>
<div id="references-1" class="section level3">
<h3>References</h3>
<ul>
<li>Altosaar, Jaan. (2016). Tutorial - What is a Variational Autoencoder?. Zenodo. <a href="https://doi.org/10.5281/zenodo.4462916" class="uri">https://doi.org/10.5281/zenodo.4462916</a></li>
<li>Dertat, A. (2017). Applied Deep Learning - Part 3: Autoencoders. Towards Data Science. <a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798" class="uri">https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798</a></li>
<li>Doersch, C. (2016). Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908.</li>
<li>Gómez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T.D., Adams, R.P., &amp; Aspuru-Guzik, A. (2018). Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2), 268-276.</li>
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. MIT press.</li>
<li>Jordan, J. (2018). Introduction to autoencoders. <a href="https://www.jeremyjordan.me/autoencoders/" class="uri">https://www.jeremyjordan.me/autoencoders/</a></li>
<li>Jordan, J. (2018). Variational autoencoders. <a href="https://www.jeremyjordan.me/variational-autoencoders/" class="uri">https://www.jeremyjordan.me/variational-autoencoders/</a></li>
<li>Lawton, G. (2020). How to troubleshoot 8 common autoencoder limitations. TechTarget. <a href="https://www.techtarget.com/searchenterpriseai/feature/How-to-troubleshoot-8-common-autoencoder-limitations" class="uri">https://www.techtarget.com/searchenterpriseai/feature/How-to-troubleshoot-8-common-autoencoder-limitations</a></li>
<li>Prendin, F., Del Favero, S., Vettoretti, M., Sparacino, G., &amp; Facchinetti, A. (2021). Forecasting of glucose levels and hypoglycemic events: head-to-head comparison of linear and nonlinear data-driven algorithms based on continuous glucose monitoring data only. Sensors, 21(5), 1647.</li>
<li>Rocca, J. (2019). Understanding Variational Autoencoders (VAEs). Towards Data Science. <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></li>
</ul>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
